# this file is running on the composer environment to backup the bigquery tables to gcs 
# send slack message to the channel when the backup is started and completed and show the is_manual flag

from __future__ import annotations

import os
from datetime import datetime
from airflow import DAG
from airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
import json
import datetime

# ========== è¨­å®šåƒæ•¸ ==========
BQ_TABLE_BOOKS = "online-bookstore-449113.bookstore_raw.books"  # BigQuery è³‡æ–™è¡¨
BQ_TABLE_TRANSACTIONS = "online-bookstore-449113.bookstore_raw.transactions"  # BigQuery è³‡æ–™è¡¨
BQ_TABLE_USERS = "online-bookstore-449113.bookstore_raw.users"  # BigQuery è³‡æ–™è¡¨

GCS_BUCKET = "gs://online-bookstore/data_from_bq"  # GCS å­˜å„²æ¡¶
GCS_FILE_PATH_BOOKS = f"{GCS_BUCKET}/{BQ_TABLE_BOOKS.split('.')[-1]}_backup_{datetime.datetime.now().strftime('%Y%m%d')}.csv"
GCS_FILE_PATH_TRANSACTIONS = f"{GCS_BUCKET}/{BQ_TABLE_TRANSACTIONS.split('.')[-1]}_backup_{datetime.datetime.now().strftime('%Y%m%d')}.csv"
GCS_FILE_PATH_USERS = f"{GCS_BUCKET}/{BQ_TABLE_USERS.split('.')[-1]}_backup_{datetime.datetime.now().strftime('%Y%m%d')}.csv"

# ç’°å¢ƒè®Šæ•¸ (å¯åœ¨ Cloud Composer ç’°å¢ƒè®Šæ•¸è¨­å®š)
SLACK_WEBHOOK_CONN_ID = os.environ.get("SLACK_WEBHOOK_CONN_ID", "slack_default")
SLACK_WEBHOOK_URL = "https://hooks.slack.com/services/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"

# ========== å®šç¾©ç™¼é€ Slack è¨Šæ¯çš„å‡½æ•¸ ==========
def get_trigger_type(**kwargs):
    """å–å¾—è§¸ç™¼æ–¹å¼ï¼šæ‰‹å‹• (manual) æˆ– æŽ’ç¨‹ (scheduled)"""
    dag_run = kwargs.get("dag_run")
    is_manual = dag_run.run_id.startswith("manual__")
    return "æ‰‹å‹•åŸ·è¡Œ (Manually Triggered)" if is_manual else "æŽ’ç¨‹åŸ·è¡Œ (Scheduled Run)"

def send_slack_start(**kwargs):
    """ç™¼é€ Slack é–‹å§‹é€šçŸ¥ï¼ŒåŒ…å«è§¸ç™¼æ–¹å¼"""
    trigger_type = get_trigger_type(**kwargs)
    execution_date = kwargs["execution_date"].strftime('%Y-%m-%d %H:%M:%S')

    slack_start = SlackWebhookOperator(
        task_id="slack_start_notification",
        slack_webhook_conn_id=SLACK_WEBHOOK_CONN_ID,
        message=(
            f":rocket: *BigQuery å‚™ä»½ä»»å‹™é–‹å§‹*\n"
            f"ðŸ“… åŸ·è¡Œæ™‚é–“: `{execution_date}`\n"
            f"ðŸ”„ è§¸ç™¼æ–¹å¼: `{trigger_type}`\n"
            f"ðŸ’¾ ç›®æ¨™ GCS: `{GCS_BUCKET}`"
        ),
    )
    slack_start.execute(kwargs)

def send_slack_end(**kwargs):
    """ç™¼é€ Slack çµæŸé€šçŸ¥"""
    trigger_type = get_trigger_type(**kwargs)
    execution_date = kwargs["execution_date"].strftime('%Y-%m-%d %H:%M:%S')

    slack_end = SlackWebhookOperator(
        task_id="slack_end_notification",
        slack_webhook_conn_id=SLACK_WEBHOOK_CONN_ID,
        message=(
            f":white_check_mark: *BigQuery å‚™ä»½å®Œæˆ*\n"
            f"ðŸ“… å®Œæˆæ™‚é–“: `{execution_date}`\n"
            f"ðŸ”„ è§¸ç™¼æ–¹å¼: `{trigger_type}`\n"
            f"âœ… å‚™ä»½ä½ç½®: `{GCS_BUCKET}`"
        ),
    )
    slack_end.execute(kwargs)

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": days_ago(1),
    "retries": 1,
}

with DAG(
    "bq_backup_to_gcs",
    default_args=default_args,
    schedule_interval="0 2 * * *",  # æ¯å¤©å‡Œæ™¨ 2 é»žåŸ·è¡Œ
    catchup=False,
    description="æ¯æ—¥å‚™ä»½ BigQuery è³‡æ–™åˆ° GCS ä¸¦ç™¼é€ Slack é€šçŸ¥",
) as dag:

    # ========== é€å‡º Slack é–‹å§‹é€šçŸ¥ ==========
    start_slack_notification = PythonOperator(
        task_id="start_slack_notification",
        python_callable=send_slack_start,
        provide_context=True,
    )

    # ========== BigQuery å‚™ä»½åˆ° GCS ==========
    export_bq_to_gcs_books = BigQueryToGCSOperator(
        task_id="export_bq_to_gcs_books",
        source_project_dataset_table=BQ_TABLE_BOOKS,
        destination_cloud_storage_uris=[GCS_FILE_PATH_BOOKS],
        export_format="CSV",
        field_delimiter=",",
        print_header=True,
    )

    export_bq_to_gcs_transactions = BigQueryToGCSOperator(
        task_id="export_bq_to_gcs_transactions",
        source_project_dataset_table=BQ_TABLE_TRANSACTIONS,
        destination_cloud_storage_uris=[GCS_FILE_PATH_TRANSACTIONS],
        export_format="CSV",
        field_delimiter=",",
        print_header=True,
    )

    export_bq_to_gcs_users = BigQueryToGCSOperator(
        task_id="export_bq_to_gcs_users",
        source_project_dataset_table=BQ_TABLE_USERS,
        destination_cloud_storage_uris=[GCS_FILE_PATH_USERS],
        export_format="CSV",
        field_delimiter=",",
        print_header=True,
    )

    # ========== é€å‡º Slack çµæŸé€šçŸ¥ ==========
    end_slack_notification = PythonOperator(
        task_id="end_slack_notification",
        python_callable=send_slack_end,
        provide_context=True,
    )

    # ========== è¨­å®š DAG ä»»å‹™é †åº ==========
    start_slack_notification >> export_bq_to_gcs_books >> export_bq_to_gcs_transactions >> export_bq_to_gcs_users >> end_slack_notification
